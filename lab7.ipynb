{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: RDD\n",
    "\n",
    "In this lab, we will be learning how to visualize and estimate a policy project using a regression discontinuity design (RDD). We will be using replication data from [de Benedictis-Kessner and Warshaw (2016)](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/WSJX0X&version=1.0) which estimates the effect of Democratic mayors on local government finances, most notably, total expenditures per capita. The outcome variable is total municipal expenditures per capita two years after the election (*Total.Expenditure.D2*), and the running variable is the Democratic candidate's vote share relative to the Republican candidate, centered.\n",
    "\n",
    "## Install (if needed) and load some specialized packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Installing packages into 'C:/Users/bowen/AppData/Local/R/win-library/4.2'\n",
      "(as 'lib' is unspecified)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "package 'dataverse' successfully unpacked and MD5 sums checked\n",
      "package 'rddensity' successfully unpacked and MD5 sums checked\n",
      "package 'rdrobust' successfully unpacked and MD5 sums checked\n",
      "package 'rdd' successfully unpacked and MD5 sums checked\n",
      "package 'stargazer' successfully unpacked and MD5 sums checked\n",
      "\n",
      "The downloaded binary packages are in\n",
      "\tC:\\Users\\bowen\\AppData\\Local\\Temp\\RtmpYz3tYg\\downloaded_packages\n"
     ]
    }
   ],
   "source": [
    "install.packages(c(\"tidyverse\", \"haven\",\"dataverse\", \"rddensity\", \"rdrobust\", \"rdd\", \"stargazer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "library(haven)\n",
    "library(dataverse)\n",
    "library(rddensity)\n",
    "library(rdrobust)\n",
    "library(rdd)\n",
    "library(stargazer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "\n",
    "We will use the **{dataverse}** package to access the replication data for the paper. One little note: the authors store their data as Rdata files, which is a file type storing the entire R workspace. Rdata files cannot be directly loaded as a data frame in R, because such files can contain functions, lists, objects, multiple data frames, etc. The function we have used in previous labs, **get_dataframe_by_doi()** we can download the data and then load the file into R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'data2'</li><li>'raw_data'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'data2'\n",
       "\\item 'raw\\_data'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'data2'\n",
       "2. 'raw_data'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"data2\"    \"raw_data\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get file as raw binary data from replication dataverse\n",
    "raw_data <- get_file_by_doi(filedoi = \"doi:10.7910/DVN/WSJX0X/PQKBMK\", # uses dataverse package\n",
    "                            server = \"dataverse.harvard.edu\")\n",
    "\n",
    "# save binary file\n",
    "writeBin(raw_data, \"mayors.RData\")\n",
    "# open workspace in R\n",
    "load(\"mayors.Rdata\")\n",
    "\n",
    "# our data is called \"data2\". See?\n",
    "ls()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic visualization and estimation\n",
    "\n",
    "Before we turn to some user-generated programs, let's first visualize and model de Benedictis-Kessner and Warshaw's using the tools we already have, **{ggplot}** and **lm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# provide the scatterplot framework for the rest of the graph and store it\n",
    "# ylim() will let us limit the y axis range\n",
    "# geom_vline() will place a vertical line on the plot\n",
    "\n",
    "p1 <- ggplot(data = data2, aes(x = demshare, y = Total.Expenditure.D2)) +\n",
    "        geom_point(color=\"gray20\", alpha = .1) +\n",
    "        theme_minimal() + ylim(-1000,1000) + \n",
    "  geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\") \n",
    "\n",
    "p1\n",
    "\n",
    "# now let's add linear fitted lines before and after the cutpoint\n",
    "p1 + geom_smooth(method = \"lm\", data = subset(data2, demshare<0), color = \"navy\") +\n",
    "     geom_smooth(method = \"lm\", data = subset(data2, demshare>0), color = \"navy\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad. Now let's plot with polynomials of the running variable, which will graph non-linearities before and after the cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# quadratic polynomial\n",
    "p1 + geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), data = subset(data2, demshare<0), color = \"navy\") +\n",
    "     geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), data = subset(data2, demshare>0), color = \"navy\")\n",
    "\n",
    "# cubic polynomial\n",
    "p1 + geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), data = subset(data2, demshare<0), color = \"navy\") +\n",
    "     geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), data = subset(data2, demshare>0), color = \"navy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization is nice, but we really want to estimate the model to see the precise estimate of the local average treatment effect (LATE) at the cutoff. First, let's create a treatment variable to use in a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data2 <- data2 |> mutate(treatment = case_when(demshare>0 ~ 1, demshare<0 ~ 0, TRUE ~ NA_real_))\n",
    "\n",
    "m1 <- lm(Total.Expenditure.D2 ~ demshare + treatment, data = data2)\n",
    "summary(m1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. Not seeing much of an effect here. Only an estimate of an additional $17 per capita increase, which is nearly half the size of the standard error. How about we add some additional flexibility into the estimation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first, allow linear trend to vary before and after the cutoff using an interaction\n",
    "m2 <- lm(Total.Expenditure.D2 ~ demshare*treatment, data = data2)\n",
    "\n",
    "# now, let's restrict the bandwidth\n",
    "m3 <- lm(Total.Expenditure.D2 ~ demshare*treatment, data = data2 |> filter(demshare>-.1 & demshare <.1))\n",
    "# and how about a quadratic polynomial\n",
    "m4 <- lm(Total.Expenditure.D2 ~ poly(demshare,p=2)*treatment, data = data2)\n",
    "# poly + restricted bandwidth\n",
    "m5 <- lm(Total.Expenditure.D2 ~ poly(demshare,p=2)*treatment, data = data2 |> filter(demshare>-.1 & demshare <.1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the **{stargazer}** package to make the regression table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "stargazer(m1, m2, m3, m4, m5, type = \"text\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how much the LATE varies by modelling decisions!\n",
    "\n",
    "The table is nice enough, but we can clean it up even more. Remember, the only information of interest is the coefficient on the treatment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "stargazer(m1, m2, m3, m4, m5, type = \"text\", omit = c(\"poly\", \"demshare\", \"demshare:treatment\"))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RDD packages to choose optimal bandwidths\n",
    "\n",
    "The **{rdrobust}** will automate the process of identifying bandwidths, although you can certainly still show your audience multiple bandwidths if you want. The package will also let you make some other choices (like using kernel regression to create the local regression estimates). As this literature discusses, you can choose among various kernels. de Benedictis-Kessner and Warshaw use a uniform kernel (equally weighted observations inside the bandwidth on either side of the cutoff) or the triangular kernel (observations closer to the cutoff are weighted more). We again can model with linear or polynomial versions of the running variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first for the linear versions\n",
    "# c = 0 is the argument for the location of the cutoff. 0 is the default.\n",
    "# uniform kernel first, and then followed by the triangular kernel next\n",
    "rd1 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, c = 0, kernel = \"uni\", all = TRUE)\n",
    "rd2 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, c = 0, kernel = \"tri\", all = TRUE)\n",
    "summary(rd1)\n",
    "summary(rd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# the p argument allows you to specify higher-order polynomials. 2 would be quadratic, 3 would be cubic.\n",
    "\n",
    "rd3 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, c = 0, p=2, kernel = \"uni\", all = TRUE)\n",
    "rd4 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, c = 0, p=2, kernel = \"tri\", all = TRUE)\n",
    "summary(rd3)\n",
    "summary(rd4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **{rdrobust}** package comes with nice default way to plot the discontinuity without all the work we did earlier. By default, the *rdplot* function will bin the data to make a cleaner graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rdplot(data2$Total.Expenditure.D2, data2$demshare, c = 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the continuity of the running variable\n",
    "\n",
    "McCrary (2008) recommends a statistical test to see if the running variable itself is discontinuous at the cutoff. Remember, it shouldn't be. Discontinuity could be a signal the units are manipulating the assignment threshold in some fashion. The **{rdd}** package includes functions to run and plot the McCrary density test. Uh oh - looks like de Benedictis-Kessner and Warshaw's data fail the test (significant discontinity of the running variable's density at the cutpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "DCdensity(data2$demshare, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
