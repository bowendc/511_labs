{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Control and Regression Discontinuities\n",
    "\n",
    "While SC and RD designs are useful in quite different data situations, both methods utilize careful comparisons to create counterfactuals and heavilyt utilize graphs to illustrate those comparisons. Both methods have a suite of specialized packages which make using the methods easier in R. \n",
    "\n",
    "## Synthetic Control\n",
    "\n",
    "Again, the synthetic control method is useful for case studies or other situtions in which you have few treated observations and are particularly interested in the effect of policy on a particular treated unit. SC uses matching and weighting to make a \"synthetic\" unit as similar as possible to our pre-treatment unit. Then we can see if the synthetic and actual units diverge post treament as evidence of an effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# install packages as needed\n",
    "# the key Synthetic Control packages are Synth and SCtools\n",
    "# we need to install (and load) devtools in order to install\n",
    "# SCtools from Github\n",
    "\n",
    "install.packages(c('tidyverse', 'Synth', 'devtools', 'cspp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "library(tidyverse)\n",
    "library(Synth)\n",
    "library(devtools)\n",
    "install_github(\"bcastanho/SCtools\")\n",
    "library(SCtools)\n",
    "library(cspp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the potential effect on state finances by limiting teacher union bargaining power through fee collection. Oklahoma's agency fee provision law went into effect in 2001. What impact might such limitation have on state finances? Perhaps it reduced state debt by limited the power of teacher unions to effectively bargain for benefits like pensions and salary increases. We can use *synthetic control* to test for the effect of the policy. Synthetic control creates, well, a synthetic control Oklahoma as a weighted average of other states. We can then compare synthetic OK to actual OK.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# get the data from the Correlates of State Policy Project\n",
    "data <- get_cspp_data(years = seq(1990,2015))\n",
    "\n",
    "df <- as.data.frame(data) # Synth requires data.frame format, not tibble! \n",
    "                          # Be really careful with this one. If you try to load \n",
    "                          # the tibble format, Synth will through a weird error \n",
    "                          # message that you won't recognize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# most of the work is done in this dataprep() call. Basically, we need to tell \n",
    "# R what predictors of state debt to match on, what the outcome or dependent variable\n",
    "# is, the time frame, and the potential units to be used for the control population.\n",
    "\n",
    "dataprep.out <- \n",
    "    dataprep(\n",
    "      foo = df,\n",
    "      predictors = c(\"disposable_personal_income1000s_annual\", \"agovempr\", \"taxes_gsp\"),\n",
    "      predictors.op = \"mean\",                 # could use diff stat\n",
    "      dependent = \"total_debt_outstanding_gsp\", # outcome variable here\n",
    "      unit.variable = \"state_fips\", # must be numeric\n",
    "      time.variable = \"year\",   # your time variable goes here \n",
    "      special.predictors = list( # this are lets you match on variables in specific time periods. \n",
    "        list(\"total_debt_outstanding_gsp\", 1990, \"mean\"),\n",
    "        list(\"total_debt_outstanding_gsp\", 2000, \"mean\"),\n",
    "        list(\"hou_chamber\", 1998, \"mean\"), # estimate of legislative ideology\n",
    "        list(\"hou_chamber\", 2000, \"mean\")), # in two different elections\n",
    "      treatment.identifier = \"OK\",  # treated unit goes here. Use \n",
    "      unit.names.variable = \"st\", # we need this if we want to specify control units below with abbreviations, otherwise we could use the state_fips codes from above\n",
    "      controls.identifier = c(\"CA\",\"CO\",\"NM\", \"PA\", \"OH\", \"NY\", \"MO\", \"MA\", \"MT\", \"KY\", \"WA\", \"WV\"), # we list our control units here\n",
    "      time.predictors.prior = c(1990:2000), # pre-implementation period\n",
    "      time.optimize.ssr = c(1990:2000),\n",
    "      time.plot = c(1990:2010)              # entire time period\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have prepared our data and described how we would like our synthetic unit to be created, we can create the data and evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# use our prepared data to create synthetic control unit\n",
    "# notice our data is now dataprep.out \n",
    "synth_out <- synth(data.prep.obj = dataprep.out)\n",
    "\n",
    "# plot synthetic and actual units \n",
    "path.plot(synth_out, dataprep.out)\n",
    "\n",
    "# plot difference between synthetic and actual\n",
    "gaps.plot(synth_out, dataprep.out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the weights created by **Synth** using `synth.tab`. This is crucial - it shows us how well our synthetic unit matches the actual OK and describes the relative weighting of each predictor variable and control units. How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "tables <- synth.tab(synth_out, dataprep.out)\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placebo analysis plays a big role here. We see that there is a difference between OK and synthetic OK, but it is enough to matter? Well, we can re-run this analysis, replacing OK with our control unit states. If we see similar movement from those states, than we probably don't have a causal effect for OK, because the other states didn't adopt the policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# the generate.placebos function will do it for us.\n",
    "placebos <- generate.placebos(dataprep.out, synth_out)\n",
    "\n",
    "plot_placebos(placebos)\n",
    "\n",
    "# mspe.plot shows post to pre-treatment prediction error for \n",
    "# actual analysis and placebos. Again, we're looking for evidence\n",
    "# that OK is unique with a higher ratio (divergence) after treatment.\n",
    "# do we see that here?\n",
    "mspe.plot(placebos, discard.extreme = TRUE, mspe.limit = 2, plot.hist = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Discontinuity (RD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be learning how to visualize and estimate a policy project using a regression discontinuity design (RDD). We will be using replication data from [de Benedictis-Kessner and Warshaw (2016)](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/WSJX0X&version=1.0) which estimates the effect of Democratic mayors on local government finances, most notably, total expenditures per capita. The outcome variable is total municipal expenditures per capita two years after the election (*Total.Expenditure.D2*), and the running variable is the Democratic candidate's vote share relative to the Republican candidate, centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Install RDD packages if not already installed\n",
    "install.packages(c(\"rddensity\", \"rdrobust\", \"rdd\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "library(rddensity)\n",
    "library(rdrobust)\n",
    "library(rdd)\n",
    "library(dataverse)\n",
    "library(modelsummary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "\n",
    "We will use the **{dataverse}** package to access the replication data for the paper. One little note: the authors store their data as Rdata files, which is a file type storing the entire R workspace. Rdata files cannot be directly loaded as a data frame in R, because such files can contain functions, lists, objects, multiple data frames, etc. The function we have used in previous labs, **get_dataframe_by_doi()** won't work here. Instead, we can download the data and then load the file into R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# get file as raw binary data from replication dataverse\n",
    "as_binary <- get_file_by_doi(\n",
    "          filedoi = \"doi:10.7910/DVN/WSJX0X/PQKBMK\",\n",
    "          server = \"dataverse.harvard.edu\"\n",
    ")\n",
    "\n",
    "# save the file in our working directory\n",
    "writeBin(as_binary, \"mayors.RData\") \n",
    "\n",
    "# open the data\n",
    "load(\"mayors.RData\")\n",
    "\n",
    "# our data is stored in a dataframe called \"data2\". See?\n",
    "ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic visualization and estimation\n",
    "\n",
    "Before we turn to some user-generated programs, let's first visualize and model de Benedictis-Kessner and Warshaw's using the tools we already have, **{ggplot}** and **lm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# provide the scatterplot framework for the rest of the graph and store it\n",
    "# ylim() will let us limit the y axis range\n",
    "# geom_vline() will place a vertical line on the plot\n",
    "\n",
    "p1 <- ggplot(data = data2, aes(x = demshare, y = Total.Expenditure.D2)) +\n",
    "        geom_point(color=\"gray20\", alpha = .1) +\n",
    "        theme_minimal() + ylim(-1000,1000) + \n",
    "  geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\") \n",
    "\n",
    "p1\n",
    "\n",
    "# now let's add linear fitted lines before and after the cutpoint\n",
    "p1 + geom_smooth(method = \"lm\", data = subset(data2, demshare<0), color = \"navy\") +\n",
    "     geom_smooth(method = \"lm\", data = subset(data2, demshare>0), color = \"navy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad. Now let's plot with polynomials of the running variable, which will graph non-linearities before and after the cutoff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# quadratic polynomial\n",
    "p1 + geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), data = subset(data2, demshare<0), color = \"navy\") +\n",
    "     geom_smooth(method = \"lm\", formula = y ~ poly(x, 2), data = subset(data2, demshare>0), color = \"navy\")\n",
    "\n",
    "# cubic polynomial\n",
    "p1 + geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), data = subset(data2, demshare<0), color = \"navy\") +\n",
    "     geom_smooth(method = \"lm\", formula = y ~ poly(x, 3), data = subset(data2, demshare>0), color = \"navy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization is nice, but we really want to estimate the model to see the precise estimate of the local average treatment effect (LATE) at the cutoff. First, let's create a treatment variable to use in a regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "data2 <- data2 |> mutate(treatment = case_when(demshare>0 ~ 1, demshare<0 ~ 0, TRUE ~ NA_real_))\n",
    "\n",
    "m1 <- lm(Total.Expenditure.D2 ~ demshare + treatment, data = data2)\n",
    "summary(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm. Not seeing much of an effect here. Only an estimate of an additional $17 per capita increase, which is nearly half the size of the standard error. How about we add some additional flexibility into the estimation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first, allow linear trend to vary before and after the cutoff using an interaction\n",
    "m2 <- lm(Total.Expenditure.D2 ~ demshare*treatment, data = data2)\n",
    "\n",
    "# now, let's restrict the bandwidth\n",
    "m3 <- lm(Total.Expenditure.D2 ~ demshare*treatment, data = data2 |> filter(demshare>-.1 & demshare <.1))\n",
    "\n",
    "# and how about a quadratic polynomial\n",
    "m4 <- lm(Total.Expenditure.D2 ~ poly(demshare,p=2)*treatment, data = data2)\n",
    "\n",
    "# poly + restricted bandwidth\n",
    "m5 <- lm(Total.Expenditure.D2 ~ poly(demshare,p=2)*treatment, data = data2 |> filter(demshare>-.1 & demshare <.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the **{modelsummary}** package to make the regression table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "modelsummary(list(m1, m2, m3, m4, m5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Notice how much the LATE varies by modelling decisions!\n",
    "\n",
    "The table is nice enough, but we can clean it up even more. Remember, the only information of interest is the coefficient on the treatment variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "modelsummary(list(m1, m2, m3, m4, m5),\n",
    "            coef_map = c('treatment' = 'Treatment'),\n",
    "            stars = TRUE,                   \n",
    "            estimate = \"{estimate}{stars}\", \n",
    "            statistic = \"({std.error})\",\n",
    "            gof_map = c(\"nobs\", \"r.squared\", \"rmse\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RDD packages to choose optimal bandwidths\n",
    "\n",
    "The **{rdrobust}** will automate the process of identifying bandwidths, although you can certainly still show your audience multiple bandwidths if you want. The package will also let you make some other choices (like using kernel regression to create the local regression estimates). As this literature discusses, you can choose among various kernels. de Benedictis-Kessner and Warshaw use a uniform kernel (equally weighted observations inside the bandwidth on either side of the cutoff) or the triangular kernel (observations closer to the cutoff are weighted more). We again can model with linear or polynomial versions of the running variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# first for the linear versions\n",
    "# c = 0 is the argument for the location of the cutoff. 0 is the default.\n",
    "# uniform kernel first, and then followed by the triangular kernel next\n",
    "rd1 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, \n",
    "                c = 0, \n",
    "                kernel = \"uni\", \n",
    "                all = TRUE)\n",
    "\n",
    "rd2 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, \n",
    "                c = 0, \n",
    "                kernel = \"tri\", \n",
    "                all = TRUE)\n",
    "                \n",
    "summary(rd1)\n",
    "summary(rd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# the p argument allows you to specify higher-order polynomials. 2 would be quadratic, 3 would be cubic.\n",
    "\n",
    "rd3 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, \n",
    "                c = 0, \n",
    "                p=2, \n",
    "                kernel = \"uni\", \n",
    "                all = TRUE)\n",
    "\n",
    "rd4 <- rdrobust(data2$Total.Expenditure.D2, data2$demshare, \n",
    "                c = 0, \n",
    "                p=2, \n",
    "                kernel = \"tri\", \n",
    "                all = TRUE)\n",
    "                \n",
    "summary(rd3)\n",
    "summary(rd4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **{rdrobust}** package comes with nice default way to plot the discontinuity without all the work we did earlier. By default, the *rdplot* function will bin the data to make a cleaner graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rdplot(data2$Total.Expenditure.D2, data2$demshare, c = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the continuity of the running variable\n",
    "\n",
    "McCrary (2008) recommends a statistical test to see if the running variable itself is discontinuous at the cutoff. Remember, it shouldn't be. Discontinuity could be a signal the units are manipulating the assignment threshold in some fashion. The **{rdd}** package includes functions to run and plot the McCrary density test. Uh oh - looks like de Benedictis-Kessner and Warshaw's data fail the test (significant discontinity of the running variable's density at the cutpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "DCdensity(data2$demshare, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
